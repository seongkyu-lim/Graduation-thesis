{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YOLOv5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMxe91Qv11NEH2+payNWGfM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seongkyu-lim/Graduation-thesis/blob/master/YOLOv5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-fP7Sa-zc1F",
        "outputId": "4a51d840-503b-4fa0-d9c2-444b9e78f256"
      },
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt  # install dependencies\n",
        "\n",
        "import torch\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "clear_output()\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Using torch 1.9.0+cu102 (Tesla T4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2McCEnNi39Kn",
        "outputId": "34913ca7-e223-4806-ca0e-1b6d490e8b79"
      },
      "source": [
        "# mount gdrive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgPRo9xX9_4i"
      },
      "source": [
        "#Inference\n",
        "Ïù¥ÎØ∏ trainedÎêú Î™®Îç∏Ïù∏ yolov5x,s,m,l Ï§ë ÏÑ±Îä•Ïù¥ Í∞ÄÏû• Ï¢ãÏùÄ xlargeÎ™®Îç∏Î°ú inference Ìï¥Î≥¥ÏïòÏäµÎãàÎã§."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgK_Du19k3sl",
        "outputId": "cde8a2ba-ecfe-48eb-b7d9-3630adc3e9ef"
      },
      "source": [
        "import torch\n",
        "\n",
        "%ls\n",
        "\n",
        "#Ï†àÎåÄÍ≤ΩÎ°úÎ°ú ÏùøÎ†•Ìï¥Ï£ºÏñ¥Ïïº ÌååÏùº ÏúÑÏπò Ïù∏ÏãùÌïúÎã§.\n",
        "!python /content/yolov5/detect.py --weight yolov5x.pt --img 1280 --conf 0.4 --save-conf --save-txt --save-crop --source /content/yolov5/data/images/train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CONTRIBUTING.md  Dockerfile  LICENSE    \u001b[0m\u001b[01;32mrequirements.txt\u001b[0m*  \u001b[01;34mutils\u001b[0m/\n",
            "\u001b[01;34mdata\u001b[0m/            export.py   \u001b[01;34mmodels\u001b[0m/    train.py           val.py\n",
            "detect.py        hubconf.py  README.md  tutorial.ipynb\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 31.8MB/s]\n",
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['yolov5x.pt'], source=/content/yolov5/data/images/train, imgsz=[1280, 1280], conf_thres=0.4, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=True, save_conf=True, save_crop=True, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False\n",
            "YOLOv5 üöÄ v5.0-438-g27a4736 torch 1.9.0+cu102 CUDA:0 (Tesla T4, 15109.75MB)\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5x.pt to yolov5x.pt...\n",
            "100% 168M/168M [00:01<00:00, 93.1MB/s]\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 476 layers, 87730285 parameters, 0 gradients\n",
            "image 1/45 /content/yolov5/data/images/train/frame0.jpg: 736x1280 9 persons, Done. (0.125s)\n",
            "image 2/45 /content/yolov5/data/images/train/frame1.jpg: 736x1280 14 persons, Done. (0.123s)\n",
            "image 3/45 /content/yolov5/data/images/train/frame10.jpg: 736x1280 10 persons, 1 chair, Done. (0.124s)\n",
            "image 4/45 /content/yolov5/data/images/train/frame11.jpg: 736x1280 11 persons, 2 chairs, Done. (0.123s)\n",
            "image 5/45 /content/yolov5/data/images/train/frame12.jpg: 736x1280 2 persons, Done. (0.124s)\n",
            "image 6/45 /content/yolov5/data/images/train/frame13.jpg: 736x1280 2 persons, Done. (0.124s)\n",
            "image 7/45 /content/yolov5/data/images/train/frame14.jpg: 736x1280 2 persons, Done. (0.125s)\n",
            "image 8/45 /content/yolov5/data/images/train/frame15.jpg: 736x1280 3 persons, Done. (0.125s)\n",
            "image 9/45 /content/yolov5/data/images/train/frame16.jpg: 736x1280 14 persons, 1 sink, Done. (0.126s)\n",
            "image 10/45 /content/yolov5/data/images/train/frame17.jpg: 736x1280 19 persons, 1 baseball glove, Done. (0.126s)\n",
            "image 11/45 /content/yolov5/data/images/train/frame18.jpg: 736x1280 20 persons, 1 baseball glove, Done. (0.125s)\n",
            "image 12/45 /content/yolov5/data/images/train/frame19.jpg: 736x1280 21 persons, Done. (0.125s)\n",
            "image 13/45 /content/yolov5/data/images/train/frame2.jpg: 736x1280 14 persons, Done. (0.124s)\n",
            "image 14/45 /content/yolov5/data/images/train/frame20.jpg: 736x1280 22 persons, 1 sports ball, 1 baseball glove, Done. (0.125s)\n",
            "image 15/45 /content/yolov5/data/images/train/frame21.jpg: 736x1280 22 persons, 1 sports ball, Done. (0.124s)\n",
            "image 16/45 /content/yolov5/data/images/train/frame22.jpg: 736x1280 21 persons, 1 sports ball, Done. (0.124s)\n",
            "image 17/45 /content/yolov5/data/images/train/frame23.jpg: 736x1280 21 persons, 1 sports ball, 1 baseball glove, Done. (0.125s)\n",
            "image 18/45 /content/yolov5/data/images/train/frame24.jpg: 736x1280 20 persons, 1 sports ball, Done. (0.124s)\n",
            "image 19/45 /content/yolov5/data/images/train/frame25.jpg: 736x1280 18 persons, 1 sports ball, Done. (0.124s)\n",
            "image 20/45 /content/yolov5/data/images/train/frame26.jpg: 736x1280 18 persons, 2 sports balls, Done. (0.124s)\n",
            "image 21/45 /content/yolov5/data/images/train/frame27.jpg: 736x1280 16 persons, 1 sports ball, Done. (0.124s)\n",
            "image 22/45 /content/yolov5/data/images/train/frame28.jpg: 736x1280 14 persons, 2 baseball gloves, Done. (0.124s)\n",
            "image 23/45 /content/yolov5/data/images/train/frame29.jpg: 736x1280 15 persons, 1 baseball glove, Done. (0.124s)\n",
            "image 24/45 /content/yolov5/data/images/train/frame3.jpg: 736x1280 16 persons, Done. (0.124s)\n",
            "image 25/45 /content/yolov5/data/images/train/frame30.jpg: 736x1280 14 persons, Done. (0.125s)\n",
            "image 26/45 /content/yolov5/data/images/train/frame31.jpg: 736x1280 13 persons, Done. (0.125s)\n",
            "image 27/45 /content/yolov5/data/images/train/frame32.jpg: 736x1280 13 persons, Done. (0.125s)\n",
            "image 28/45 /content/yolov5/data/images/train/frame33.jpg: 736x1280 15 persons, 1 tennis racket, Done. (0.124s)\n",
            "image 29/45 /content/yolov5/data/images/train/frame34.jpg: 736x1280 16 persons, Done. (0.123s)\n",
            "image 30/45 /content/yolov5/data/images/train/frame35.jpg: 736x1280 14 persons, Done. (0.126s)\n",
            "image 31/45 /content/yolov5/data/images/train/frame36.jpg: 736x1280 13 persons, Done. (0.125s)\n",
            "image 32/45 /content/yolov5/data/images/train/frame37.jpg: 736x1280 15 persons, 1 sports ball, Done. (0.124s)\n",
            "image 33/45 /content/yolov5/data/images/train/frame38.jpg: 736x1280 14 persons, 1 sports ball, 1 baseball glove, Done. (0.126s)\n",
            "image 34/45 /content/yolov5/data/images/train/frame39.jpg: 736x1280 13 persons, 1 sports ball, 1 tennis racket, Done. (0.125s)\n",
            "image 35/45 /content/yolov5/data/images/train/frame4.jpg: 736x1280 16 persons, Done. (0.125s)\n",
            "image 36/45 /content/yolov5/data/images/train/frame40.jpg: 736x1280 16 persons, 1 sports ball, 1 baseball glove, 2 tennis rackets, Done. (0.127s)\n",
            "image 37/45 /content/yolov5/data/images/train/frame41.jpg: 736x1280 14 persons, 1 sports ball, 1 tennis racket, Done. (0.124s)\n",
            "image 38/45 /content/yolov5/data/images/train/frame42.jpg: 736x1280 13 persons, Done. (0.123s)\n",
            "image 39/45 /content/yolov5/data/images/train/frame43.jpg: 736x1280 11 persons, 1 sports ball, Done. (0.124s)\n",
            "image 40/45 /content/yolov5/data/images/train/frame44.jpg: 736x1280 16 persons, 1 sports ball, Done. (0.124s)\n",
            "image 41/45 /content/yolov5/data/images/train/frame5.jpg: 736x1280 15 persons, Done. (0.125s)\n",
            "image 42/45 /content/yolov5/data/images/train/frame6.jpg: 736x1280 16 persons, Done. (0.126s)\n",
            "image 43/45 /content/yolov5/data/images/train/frame7.jpg: 736x1280 14 persons, Done. (0.124s)\n",
            "image 44/45 /content/yolov5/data/images/train/frame8.jpg: 736x1280 13 persons, Done. (0.125s)\n",
            "image 45/45 /content/yolov5/data/images/train/frame9.jpg: 736x1280 14 persons, 1 chair, Done. (0.126s)\n",
            "Speed: 0.9ms pre-process, 124.6ms inference, 1.7ms NMS per image at shape (1, 3, 1280, 1280)\n",
            "Results saved to \u001b[1mruns/detect/exp\u001b[0m\n",
            "45 labels saved to runs/detect/exp/labels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH-e5Od7-cmr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "eb2dc09c-c7fe-4a40-8b38-cd747161a7f1"
      },
      "source": [
        "# Ïù¥ÎØ∏ÏßÄÎ°ú ÌôïÏù∏Ìï¥Î≥¥Í∏∞. \n",
        "# ÏòÅÏÉÅÏúºÎ°ú Ï†ÄÏû•ÎêòÏÑú Î∂àÍ∞ÄÎä•.\n",
        "Image(filename='runs/detect/exp/', width=600)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-61d069bb73cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Ïù¥ÎØ∏ÏßÄÎ°ú ÌôïÏù∏Ìï¥Î≥¥Í∏∞.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'runs/detect/exp/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ACCEPTABLE_EMBEDDINGS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot embed the '%s' image format\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1016\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot embed the 'runs/detect/exp/' image format"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQPXXNVS-ixd"
      },
      "source": [
        "# inferenceÎêú ÏòÅÏÉÅ Îã§Ïö¥Î°úÎìú Î∞õÍ∏∞.\n",
        "from google.colab import files\n",
        "files.download('./inference/output/ARS-CHEL-0.mp4')\n",
        "\n",
        "# ÎòêÎäî Íµ¨Í∏ÄÎìúÎùºÏù¥Î∏åÎ°ú ÏòÆÍ≤®ÏÑú ÌôïÏù∏ÌïòÍ∏∞."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NfRwHhu-555"
      },
      "source": [
        "#Train\n",
        "\n",
        "  trainÌïòÍ∏∞\n",
        "\n",
        "##Challenge\n",
        "\n",
        "- Ïñë ÌåÄÏùÑ Îã§Î•¥Í≤å Ïù∏ÏãùÌï¥ÏïºÌïúÎã§.\n",
        "- Ïã¨ÌåêÍ≥º Í≥µÎòêÌïú Ïù∏ÏãùÌï¥ÏïºÌïúÎã§."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Bv2ETKPbnby"
      },
      "source": [
        "##1.opencvÎ•º ÌôúÏö©Ìïú ÏòÅÏÉÅ Ï≤òÎ¶¨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWWYgNVXbyAj",
        "outputId": "7ba19a4c-2cdb-45d7-9539-a545bd67b73f"
      },
      "source": [
        "%pip install opencv-python\n",
        "%pip install numpy\n",
        "%pip install matplotlib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.19.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oRZMJE4hbzNu",
        "outputId": "b513e627-d94f-412d-d077-3082104164b2"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "__author__ = 'Seran'\n",
        " \n",
        "import cv2\n",
        " \n",
        "# ÏòÅÏÉÅÏùò ÏùòÎØ∏ÏßÄÎ•º Ïó∞ÏÜçÏ†ÅÏúºÎ°ú Ï∫°Ï≥êÌï† Ïàò ÏûàÍ≤å ÌïòÎäî class\n",
        "vidcap = cv2.VideoCapture('/content/drive/MyDrive/Ï°∏ÏóÖÎÖºÎ¨∏/20210822-ARS-CHE-EPL_1-1080.mkv')\n",
        " \n",
        "count = 0\n",
        "\n",
        "while(vidcap.isOpened()):\n",
        "    ret, image = vidcap.read()\n",
        " \n",
        "    if(int(vidcap.get(1)) % 200 == 0):\n",
        "        print('Saved frame number : ' + str(int(vidcap.get(1))))\n",
        "        cv2.imwrite(\"/content/yolov5/data/images/train/frame%d.jpg\" % count, image) # data/images/trainÏóê Ï†ÄÏû•.\n",
        "        print('Saved frame%d.jpg' % count)\n",
        "        count += 1\n",
        " \n",
        "vidcap.release()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved frame number : 200\n",
            "Saved frame0.jpg\n",
            "Saved frame number : 400\n",
            "Saved frame1.jpg\n",
            "Saved frame number : 600\n",
            "Saved frame2.jpg\n",
            "Saved frame number : 800\n",
            "Saved frame3.jpg\n",
            "Saved frame number : 1000\n",
            "Saved frame4.jpg\n",
            "Saved frame number : 1200\n",
            "Saved frame5.jpg\n",
            "Saved frame number : 1400\n",
            "Saved frame6.jpg\n",
            "Saved frame number : 1600\n",
            "Saved frame7.jpg\n",
            "Saved frame number : 1800\n",
            "Saved frame8.jpg\n",
            "Saved frame number : 2000\n",
            "Saved frame9.jpg\n",
            "Saved frame number : 2200\n",
            "Saved frame10.jpg\n",
            "Saved frame number : 2400\n",
            "Saved frame11.jpg\n",
            "Saved frame number : 2600\n",
            "Saved frame12.jpg\n",
            "Saved frame number : 2800\n",
            "Saved frame13.jpg\n",
            "Saved frame number : 3000\n",
            "Saved frame14.jpg\n",
            "Saved frame number : 3200\n",
            "Saved frame15.jpg\n",
            "Saved frame number : 3400\n",
            "Saved frame16.jpg\n",
            "Saved frame number : 3600\n",
            "Saved frame17.jpg\n",
            "Saved frame number : 3800\n",
            "Saved frame18.jpg\n",
            "Saved frame number : 4000\n",
            "Saved frame19.jpg\n",
            "Saved frame number : 4200\n",
            "Saved frame20.jpg\n",
            "Saved frame number : 4400\n",
            "Saved frame21.jpg\n",
            "Saved frame number : 4600\n",
            "Saved frame22.jpg\n",
            "Saved frame number : 4800\n",
            "Saved frame23.jpg\n",
            "Saved frame number : 5000\n",
            "Saved frame24.jpg\n",
            "Saved frame number : 5200\n",
            "Saved frame25.jpg\n",
            "Saved frame number : 5400\n",
            "Saved frame26.jpg\n",
            "Saved frame number : 5600\n",
            "Saved frame27.jpg\n",
            "Saved frame number : 5800\n",
            "Saved frame28.jpg\n",
            "Saved frame number : 6000\n",
            "Saved frame29.jpg\n",
            "Saved frame number : 6200\n",
            "Saved frame30.jpg\n",
            "Saved frame number : 6400\n",
            "Saved frame31.jpg\n",
            "Saved frame number : 6600\n",
            "Saved frame32.jpg\n",
            "Saved frame number : 6800\n",
            "Saved frame33.jpg\n",
            "Saved frame number : 7000\n",
            "Saved frame34.jpg\n",
            "Saved frame number : 7200\n",
            "Saved frame35.jpg\n",
            "Saved frame number : 7400\n",
            "Saved frame36.jpg\n",
            "Saved frame number : 7600\n",
            "Saved frame37.jpg\n",
            "Saved frame number : 7800\n",
            "Saved frame38.jpg\n",
            "Saved frame number : 8000\n",
            "Saved frame39.jpg\n",
            "Saved frame number : 8200\n",
            "Saved frame40.jpg\n",
            "Saved frame number : 8400\n",
            "Saved frame41.jpg\n",
            "Saved frame number : 8600\n",
            "Saved frame42.jpg\n",
            "Saved frame number : 8800\n",
            "Saved frame43.jpg\n",
            "Saved frame number : 9000\n",
            "Saved frame44.jpg\n",
            "Saved frame number : 9200\n",
            "Saved frame45.jpg\n",
            "Saved frame number : 9400\n",
            "Saved frame46.jpg\n",
            "Saved frame number : 9600\n",
            "Saved frame47.jpg\n",
            "Saved frame number : 9800\n",
            "Saved frame48.jpg\n",
            "Saved frame number : 10000\n",
            "Saved frame49.jpg\n",
            "Saved frame number : 10200\n",
            "Saved frame50.jpg\n",
            "Saved frame number : 10400\n",
            "Saved frame51.jpg\n",
            "Saved frame number : 10600\n",
            "Saved frame52.jpg\n",
            "Saved frame number : 10800\n",
            "Saved frame53.jpg\n",
            "Saved frame number : 11000\n",
            "Saved frame54.jpg\n",
            "Saved frame number : 11200\n",
            "Saved frame55.jpg\n",
            "Saved frame number : 11400\n",
            "Saved frame56.jpg\n",
            "Saved frame number : 11600\n",
            "Saved frame57.jpg\n",
            "Saved frame number : 11800\n",
            "Saved frame58.jpg\n",
            "Saved frame number : 12000\n",
            "Saved frame59.jpg\n",
            "Saved frame number : 12200\n",
            "Saved frame60.jpg\n",
            "Saved frame number : 12400\n",
            "Saved frame61.jpg\n",
            "Saved frame number : 12600\n",
            "Saved frame62.jpg\n",
            "Saved frame number : 12800\n",
            "Saved frame63.jpg\n",
            "Saved frame number : 13000\n",
            "Saved frame64.jpg\n",
            "Saved frame number : 13200\n",
            "Saved frame65.jpg\n",
            "Saved frame number : 13400\n",
            "Saved frame66.jpg\n",
            "Saved frame number : 13600\n",
            "Saved frame67.jpg\n",
            "Saved frame number : 13800\n",
            "Saved frame68.jpg\n",
            "Saved frame number : 14000\n",
            "Saved frame69.jpg\n",
            "Saved frame number : 14200\n",
            "Saved frame70.jpg\n",
            "Saved frame number : 14400\n",
            "Saved frame71.jpg\n",
            "Saved frame number : 14600\n",
            "Saved frame72.jpg\n",
            "Saved frame number : 14800\n",
            "Saved frame73.jpg\n",
            "Saved frame number : 15000\n",
            "Saved frame74.jpg\n",
            "Saved frame number : 15200\n",
            "Saved frame75.jpg\n",
            "Saved frame number : 15400\n",
            "Saved frame76.jpg\n",
            "Saved frame number : 15600\n",
            "Saved frame77.jpg\n",
            "Saved frame number : 15800\n",
            "Saved frame78.jpg\n",
            "Saved frame number : 16000\n",
            "Saved frame79.jpg\n",
            "Saved frame number : 16200\n",
            "Saved frame80.jpg\n",
            "Saved frame number : 16400\n",
            "Saved frame81.jpg\n",
            "Saved frame number : 16600\n",
            "Saved frame82.jpg\n",
            "Saved frame number : 16800\n",
            "Saved frame83.jpg\n",
            "Saved frame number : 17000\n",
            "Saved frame84.jpg\n",
            "Saved frame number : 17200\n",
            "Saved frame85.jpg\n",
            "Saved frame number : 17400\n",
            "Saved frame86.jpg\n",
            "Saved frame number : 17600\n",
            "Saved frame87.jpg\n",
            "Saved frame number : 17800\n",
            "Saved frame88.jpg\n",
            "Saved frame number : 18000\n",
            "Saved frame89.jpg\n",
            "Saved frame number : 18200\n",
            "Saved frame90.jpg\n",
            "Saved frame number : 18400\n",
            "Saved frame91.jpg\n",
            "Saved frame number : 18600\n",
            "Saved frame92.jpg\n",
            "Saved frame number : 18800\n",
            "Saved frame93.jpg\n",
            "Saved frame number : 19000\n",
            "Saved frame94.jpg\n",
            "Saved frame number : 19200\n",
            "Saved frame95.jpg\n",
            "Saved frame number : 19400\n",
            "Saved frame96.jpg\n",
            "Saved frame number : 19600\n",
            "Saved frame97.jpg\n",
            "Saved frame number : 19800\n",
            "Saved frame98.jpg\n",
            "Saved frame number : 20000\n",
            "Saved frame99.jpg\n",
            "Saved frame number : 20200\n",
            "Saved frame100.jpg\n",
            "Saved frame number : 20400\n",
            "Saved frame101.jpg\n",
            "Saved frame number : 20600\n",
            "Saved frame102.jpg\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-9ee139872013>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvidcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misOpened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvidcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvidcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss3BxYArb4a_"
      },
      "source": [
        "%rm -rf /content/yolov5/data/images/train\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWtOONSVb40i"
      },
      "source": [
        "## 2.yolov5Î°ú train ÏßÑÌñâ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnqPxuyWc7fd"
      },
      "source": [
        "# Download COCO128\n",
        "torch.hub.download_url_to_file('https://github.com/ultralytics/yolov5/releases/download/v1.0/coco128.zip', 'tmp.zip')\n",
        "!unzip -q tmp.zip -d ../datasets && rm tmp.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8fYRWV--4wX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "1805c05c-068a-4c1b-b823-8a0cbced16d9"
      },
      "source": [
        "\n",
        "'''\n",
        "# Tensorboard  (optional)\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs/train\n",
        "'''\n",
        "# Weights & Biases(wandb)  (optional)\n",
        "%pip install -q wandb\n",
        "import wandb\n",
        "wandb.login()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.7 MB 8.3 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97 kB 8.8 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170 kB 66.9 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 133 kB 76.0 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63 kB 2.2 MB/s \n",
            "\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsPscZU--40o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f64bc932-6b05-48ed-e83a-c7edbf46bde1"
      },
      "source": [
        "# dataset.yamlÏóê ÌõàÎ†®, validÎ•º ÏúÑÌïú Îç∞Ïù¥ÌÑ∞ÏÖã Ï†ÄÏû• Í≤ΩÎ°ú Ï†ïÎ≥¥ ÏûàÏùå.\n",
        "!python train.py --img 640 --batch 16 --epochs 3 --data dataset.yaml --weights yolov5x.pt --cache"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5x.pt, cfg=, data=dataset.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=3, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=runs/train, entity=None, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias=latest, local_rank=-1, freeze=0, patience=100\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ‚úÖ\n",
            "YOLOv5 üöÄ v5.0-438-g27a4736 torch 1.9.0+cu102 CUDA:0 (Tesla T4, 15109.75MB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.2, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mseongkyu\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfallen-violet-6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/seongkyu/YOLOv5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/seongkyu/YOLOv5/runs/1j70k43n\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/yolov5/wandb/run-20210917_072136-1j70k43n\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      8800  models.common.Focus                     [3, 80, 3]                    \n",
            "  1                -1  1    115520  models.common.Conv                      [80, 160, 3, 2]               \n",
            "  2                -1  4    309120  models.common.C3                        [160, 160, 4]                 \n",
            "  3                -1  1    461440  models.common.Conv                      [160, 320, 3, 2]              \n",
            "  4                -1 12   3285760  models.common.C3                        [320, 320, 12]                \n",
            "  5                -1  1   1844480  models.common.Conv                      [320, 640, 3, 2]              \n",
            "  6                -1 12  13125120  models.common.C3                        [640, 640, 12]                \n",
            "  7                -1  1   7375360  models.common.Conv                      [640, 1280, 3, 2]             \n",
            "  8                -1  1   4099840  models.common.SPP                       [1280, 1280, [5, 9, 13]]      \n",
            "  9                -1  4  19676160  models.common.C3                        [1280, 1280, 4, False]        \n",
            " 10                -1  1    820480  models.common.Conv                      [1280, 640, 1, 1]             \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  4   5332480  models.common.C3                        [1280, 640, 4, False]         \n",
            " 14                -1  1    205440  models.common.Conv                      [640, 320, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  4   1335040  models.common.C3                        [640, 320, 4, False]          \n",
            " 18                -1  1    922240  models.common.Conv                      [320, 320, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  4   4922880  models.common.C3                        [640, 640, 4, False]          \n",
            " 21                -1  1   3687680  models.common.Conv                      [640, 640, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  4  19676160  models.common.C3                        [1280, 1280, 4, False]        \n",
            " 24      [17, 20, 23]  1    571965  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [320, 640, 1280]]\n",
            "Model Summary: 607 layers, 87775965 parameters, 87775965 gradients, 219.0 GFLOPs\n",
            "\n",
            "Transferred 794/794 items from yolov5x.pt\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 131 weight, 134 weight (no decay), 134 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/yolov5/data/labels/train.cache' images and labels... 40 found, 0 missing, 0 empty, 40 corrupted: 100% 40/40 [00:00<?, ?it/s]\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame0.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame1.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame10.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame11.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame12.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame13.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame14.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame15.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame16.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame17.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame19.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame18.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame2.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame20.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame21.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame22.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame23.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame24.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame25.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame26.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame27.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame28.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame3.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame29.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame31.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame30.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame32.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame33.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame34.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame35.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame36.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame37.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame38.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame39.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame4.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame40.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame41.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame42.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame43.jpg: labels require 5 columns each\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/yolov5/data/images/train/frame44.jpg: labels require 5 columns each\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 611, in <module>\n",
            "    main(opt)\n",
            "  File \"train.py\", line 509, in main\n",
            "    train(opt.hyp, opt, device, callbacks)\n",
            "  File \"train.py\", line 210, in train\n",
            "    prefix=colorstr('train: '))\n",
            "  File \"/content/yolov5/utils/datasets.py\", line 107, in create_dataloader\n",
            "    prefix=prefix)\n",
            "  File \"/content/yolov5/utils/datasets.py\", line 422, in __init__\n",
            "    labels, shapes, self.segments = zip(*cache.values())\n",
            "ValueError: not enough values to unpack (expected 3, got 0)\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/yolov5/data/labels/train.cache' images and labels... 40 found, 0 missing, 0 empty, 40 corrupted: 100% 40/40 [00:00<?, ?it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1466\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program failed with code 1.  Press ctrl-c to abort syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/yolov5/wandb/run-20210917_072136-1j70k43n/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/yolov5/wandb/run-20210917_072136-1j70k43n/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mfallen-violet-6\u001b[0m: \u001b[34mhttps://wandb.ai/seongkyu/YOLOv5/runs/1j70k43n\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnDwKkSN-47A"
      },
      "source": [
        "%rm -rf /content/yolov5/data/labels/train \n",
        "%rm -rf /content/yiolov5/data/labels/valid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAoPKEvr-498"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "873I5mEqp_UG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}