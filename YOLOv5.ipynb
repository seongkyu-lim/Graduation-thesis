{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YOLOv5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP35/JSbsIoaWmVtU8Z6Kpp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2e72051cd910444ba294fc9e8b2f1550": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7f82ef7698404894aeb07ea98b00a03f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2530ed7c921e464f9387d64c5ced1935",
              "IPY_MODEL_8969ebbbb1aa47fa9e11fa38b94de13c",
              "IPY_MODEL_7b86a15784ce4ef2bd609d0d3b7cd4b5"
            ]
          }
        },
        "7f82ef7698404894aeb07ea98b00a03f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2530ed7c921e464f9387d64c5ced1935": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4a4afb1ce4ee45ecbfdb2caed3c30894",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4ea8e31f26f34350862a4bff48a5bfdc"
          }
        },
        "8969ebbbb1aa47fa9e11fa38b94de13c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cf4aa1b0b461498bbdf3e5e9ef60fc77",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 6984509,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 6984509,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_528551ee00eb47e599054b3f5e54d83b"
          }
        },
        "7b86a15784ce4ef2bd609d0d3b7cd4b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4d38f517161a4f76826bb2cae83ccb28",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 6.66M/6.66M [00:00&lt;00:00, 14.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4d1b146a428b4cd8a4e4a199856ccecb"
          }
        },
        "4a4afb1ce4ee45ecbfdb2caed3c30894": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4ea8e31f26f34350862a4bff48a5bfdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cf4aa1b0b461498bbdf3e5e9ef60fc77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "528551ee00eb47e599054b3f5e54d83b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4d38f517161a4f76826bb2cae83ccb28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4d1b146a428b4cd8a4e4a199856ccecb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seongkyu-lim/Graduation-thesis/blob/master/YOLOv5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-fP7Sa-zc1F",
        "outputId": "95afad05-51b4-402c-f39a-8fceb7398f5e"
      },
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt  # install dependencies\n",
        "\n",
        "import torch\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "clear_output()\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Using torch 1.9.0+cu102 (Tesla T4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2McCEnNi39Kn",
        "outputId": "27b90dbb-3c56-46a4-d994-de0798b150f7"
      },
      "source": [
        "# mount gdrive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgPRo9xX9_4i"
      },
      "source": [
        "#Inference\n",
        "이미 trained된 모델인 yolov5x,s,m,l 중 성능이 가장 좋은 xlarge모델로 inference 해보았습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgK_Du19k3sl",
        "outputId": "cde8a2ba-ecfe-48eb-b7d9-3630adc3e9ef"
      },
      "source": [
        "import torch\n",
        "\n",
        "%ls\n",
        "\n",
        "#절대경로로 읿력해주어야 파일 위치 인식한다.\n",
        "!python /content/yolov5/detect.py --weight yolov5x.pt --img 1280 --conf 0.4 --save-conf --save-txt --save-crop --source /content/yolov5/data/images/train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CONTRIBUTING.md  Dockerfile  LICENSE    \u001b[0m\u001b[01;32mrequirements.txt\u001b[0m*  \u001b[01;34mutils\u001b[0m/\n",
            "\u001b[01;34mdata\u001b[0m/            export.py   \u001b[01;34mmodels\u001b[0m/    train.py           val.py\n",
            "detect.py        hubconf.py  README.md  tutorial.ipynb\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 31.8MB/s]\n",
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['yolov5x.pt'], source=/content/yolov5/data/images/train, imgsz=[1280, 1280], conf_thres=0.4, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=True, save_conf=True, save_crop=True, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False\n",
            "YOLOv5 🚀 v5.0-438-g27a4736 torch 1.9.0+cu102 CUDA:0 (Tesla T4, 15109.75MB)\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5x.pt to yolov5x.pt...\n",
            "100% 168M/168M [00:01<00:00, 93.1MB/s]\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 476 layers, 87730285 parameters, 0 gradients\n",
            "image 1/45 /content/yolov5/data/images/train/frame0.jpg: 736x1280 9 persons, Done. (0.125s)\n",
            "image 2/45 /content/yolov5/data/images/train/frame1.jpg: 736x1280 14 persons, Done. (0.123s)\n",
            "image 3/45 /content/yolov5/data/images/train/frame10.jpg: 736x1280 10 persons, 1 chair, Done. (0.124s)\n",
            "image 4/45 /content/yolov5/data/images/train/frame11.jpg: 736x1280 11 persons, 2 chairs, Done. (0.123s)\n",
            "image 5/45 /content/yolov5/data/images/train/frame12.jpg: 736x1280 2 persons, Done. (0.124s)\n",
            "image 6/45 /content/yolov5/data/images/train/frame13.jpg: 736x1280 2 persons, Done. (0.124s)\n",
            "image 7/45 /content/yolov5/data/images/train/frame14.jpg: 736x1280 2 persons, Done. (0.125s)\n",
            "image 8/45 /content/yolov5/data/images/train/frame15.jpg: 736x1280 3 persons, Done. (0.125s)\n",
            "image 9/45 /content/yolov5/data/images/train/frame16.jpg: 736x1280 14 persons, 1 sink, Done. (0.126s)\n",
            "image 10/45 /content/yolov5/data/images/train/frame17.jpg: 736x1280 19 persons, 1 baseball glove, Done. (0.126s)\n",
            "image 11/45 /content/yolov5/data/images/train/frame18.jpg: 736x1280 20 persons, 1 baseball glove, Done. (0.125s)\n",
            "image 12/45 /content/yolov5/data/images/train/frame19.jpg: 736x1280 21 persons, Done. (0.125s)\n",
            "image 13/45 /content/yolov5/data/images/train/frame2.jpg: 736x1280 14 persons, Done. (0.124s)\n",
            "image 14/45 /content/yolov5/data/images/train/frame20.jpg: 736x1280 22 persons, 1 sports ball, 1 baseball glove, Done. (0.125s)\n",
            "image 15/45 /content/yolov5/data/images/train/frame21.jpg: 736x1280 22 persons, 1 sports ball, Done. (0.124s)\n",
            "image 16/45 /content/yolov5/data/images/train/frame22.jpg: 736x1280 21 persons, 1 sports ball, Done. (0.124s)\n",
            "image 17/45 /content/yolov5/data/images/train/frame23.jpg: 736x1280 21 persons, 1 sports ball, 1 baseball glove, Done. (0.125s)\n",
            "image 18/45 /content/yolov5/data/images/train/frame24.jpg: 736x1280 20 persons, 1 sports ball, Done. (0.124s)\n",
            "image 19/45 /content/yolov5/data/images/train/frame25.jpg: 736x1280 18 persons, 1 sports ball, Done. (0.124s)\n",
            "image 20/45 /content/yolov5/data/images/train/frame26.jpg: 736x1280 18 persons, 2 sports balls, Done. (0.124s)\n",
            "image 21/45 /content/yolov5/data/images/train/frame27.jpg: 736x1280 16 persons, 1 sports ball, Done. (0.124s)\n",
            "image 22/45 /content/yolov5/data/images/train/frame28.jpg: 736x1280 14 persons, 2 baseball gloves, Done. (0.124s)\n",
            "image 23/45 /content/yolov5/data/images/train/frame29.jpg: 736x1280 15 persons, 1 baseball glove, Done. (0.124s)\n",
            "image 24/45 /content/yolov5/data/images/train/frame3.jpg: 736x1280 16 persons, Done. (0.124s)\n",
            "image 25/45 /content/yolov5/data/images/train/frame30.jpg: 736x1280 14 persons, Done. (0.125s)\n",
            "image 26/45 /content/yolov5/data/images/train/frame31.jpg: 736x1280 13 persons, Done. (0.125s)\n",
            "image 27/45 /content/yolov5/data/images/train/frame32.jpg: 736x1280 13 persons, Done. (0.125s)\n",
            "image 28/45 /content/yolov5/data/images/train/frame33.jpg: 736x1280 15 persons, 1 tennis racket, Done. (0.124s)\n",
            "image 29/45 /content/yolov5/data/images/train/frame34.jpg: 736x1280 16 persons, Done. (0.123s)\n",
            "image 30/45 /content/yolov5/data/images/train/frame35.jpg: 736x1280 14 persons, Done. (0.126s)\n",
            "image 31/45 /content/yolov5/data/images/train/frame36.jpg: 736x1280 13 persons, Done. (0.125s)\n",
            "image 32/45 /content/yolov5/data/images/train/frame37.jpg: 736x1280 15 persons, 1 sports ball, Done. (0.124s)\n",
            "image 33/45 /content/yolov5/data/images/train/frame38.jpg: 736x1280 14 persons, 1 sports ball, 1 baseball glove, Done. (0.126s)\n",
            "image 34/45 /content/yolov5/data/images/train/frame39.jpg: 736x1280 13 persons, 1 sports ball, 1 tennis racket, Done. (0.125s)\n",
            "image 35/45 /content/yolov5/data/images/train/frame4.jpg: 736x1280 16 persons, Done. (0.125s)\n",
            "image 36/45 /content/yolov5/data/images/train/frame40.jpg: 736x1280 16 persons, 1 sports ball, 1 baseball glove, 2 tennis rackets, Done. (0.127s)\n",
            "image 37/45 /content/yolov5/data/images/train/frame41.jpg: 736x1280 14 persons, 1 sports ball, 1 tennis racket, Done. (0.124s)\n",
            "image 38/45 /content/yolov5/data/images/train/frame42.jpg: 736x1280 13 persons, Done. (0.123s)\n",
            "image 39/45 /content/yolov5/data/images/train/frame43.jpg: 736x1280 11 persons, 1 sports ball, Done. (0.124s)\n",
            "image 40/45 /content/yolov5/data/images/train/frame44.jpg: 736x1280 16 persons, 1 sports ball, Done. (0.124s)\n",
            "image 41/45 /content/yolov5/data/images/train/frame5.jpg: 736x1280 15 persons, Done. (0.125s)\n",
            "image 42/45 /content/yolov5/data/images/train/frame6.jpg: 736x1280 16 persons, Done. (0.126s)\n",
            "image 43/45 /content/yolov5/data/images/train/frame7.jpg: 736x1280 14 persons, Done. (0.124s)\n",
            "image 44/45 /content/yolov5/data/images/train/frame8.jpg: 736x1280 13 persons, Done. (0.125s)\n",
            "image 45/45 /content/yolov5/data/images/train/frame9.jpg: 736x1280 14 persons, 1 chair, Done. (0.126s)\n",
            "Speed: 0.9ms pre-process, 124.6ms inference, 1.7ms NMS per image at shape (1, 3, 1280, 1280)\n",
            "Results saved to \u001b[1mruns/detect/exp\u001b[0m\n",
            "45 labels saved to runs/detect/exp/labels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH-e5Od7-cmr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "eb2dc09c-c7fe-4a40-8b38-cd747161a7f1"
      },
      "source": [
        "# 이미지로 확인해보기. \n",
        "# 영상으로 저장되서 불가능.\n",
        "Image(filename='runs/detect/exp/', width=600)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-61d069bb73cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 이미지로 확인해보기.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'runs/detect/exp/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ACCEPTABLE_EMBEDDINGS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot embed the '%s' image format\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1016\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot embed the 'runs/detect/exp/' image format"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQPXXNVS-ixd"
      },
      "source": [
        "# inference된 영상 다운로드 받기.\n",
        "from google.colab import files\n",
        "files.download('./inference/output/ARS-CHEL-0.mp4')\n",
        "\n",
        "# 또는 구글드라이브로 옮겨서 확인하기."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NfRwHhu-555"
      },
      "source": [
        "#Train\n",
        "\n",
        "  train하기\n",
        "\n",
        "##Challenge\n",
        "\n",
        "- 양 팀을 다르게 인식해야한다.\n",
        "- 심판과 공또한 인식해야한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Bv2ETKPbnby"
      },
      "source": [
        "##1.opencv를 활용한 영상 처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWWYgNVXbyAj",
        "outputId": "470b9980-9cfc-4907-80f3-37991acccba9"
      },
      "source": [
        "%pip install opencv-python\n",
        "%pip install numpy\n",
        "%pip install matplotlib"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.19.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "oRZMJE4hbzNu",
        "outputId": "1fb33844-6db5-43eb-c96e-3c2bd4868044"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "__author__ = 'Seran'\n",
        " \n",
        "import cv2\n",
        " \n",
        "# 영상의 의미지를 연속적으로 캡쳐할 수 있게 하는 class\n",
        "vidcap = cv2.VideoCapture('/content/drive/MyDrive/졸업논문/20210822-ARS-CHE-EPL_1-1080.mkv')\n",
        " \n",
        "count = 0\n",
        "\n",
        "while(vidcap.isOpened()):\n",
        "    ret, image = vidcap.read()\n",
        "    \n",
        "    if(int(vidcap.get(1)) % 200 == 0):\n",
        "        print('Saved frame number : ' + str(int(vidcap.get(1))))\n",
        "        cv2.imwrite(\"/content/yolov5/data/images/train3/frame%d.jpg\" % count, image) # data/images/train에 저장.\n",
        "        print('Saved frame%d.jpg' % count)\n",
        "        count += 1\n",
        " \n",
        "vidcap.release()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-d9274e6f1b86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvidcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misOpened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvidcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvidcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>=\u001b[0m \u001b[0;36m80000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvidcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss3BxYArb4a_"
      },
      "source": [
        "%mv /content/yolov5/data/images/train3 /content/drive/MyDrive/졸업논문\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWtOONSVb40i"
      },
      "source": [
        "## 2.yolov5로 train 진행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnqPxuyWc7fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "2e72051cd910444ba294fc9e8b2f1550",
            "7f82ef7698404894aeb07ea98b00a03f",
            "2530ed7c921e464f9387d64c5ced1935",
            "8969ebbbb1aa47fa9e11fa38b94de13c",
            "7b86a15784ce4ef2bd609d0d3b7cd4b5",
            "4a4afb1ce4ee45ecbfdb2caed3c30894",
            "4ea8e31f26f34350862a4bff48a5bfdc",
            "cf4aa1b0b461498bbdf3e5e9ef60fc77",
            "528551ee00eb47e599054b3f5e54d83b",
            "4d38f517161a4f76826bb2cae83ccb28",
            "4d1b146a428b4cd8a4e4a199856ccecb"
          ]
        },
        "outputId": "a243c993-26fc-48f4-92b1-db397f441bb4"
      },
      "source": [
        "# Download COCO128\n",
        "torch.hub.download_url_to_file('https://github.com/ultralytics/yolov5/releases/download/v1.0/coco128.zip', 'tmp.zip')\n",
        "!unzip -q tmp.zip -d ../datasets && rm tmp.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e72051cd910444ba294fc9e8b2f1550",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/6.66M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8fYRWV--4wX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "1e4f7b9c-47e6-4df0-8c72-f5d341a5ee99"
      },
      "source": [
        "\n",
        "'''\n",
        "# Tensorboard  (optional)\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs/train\n",
        "'''\n",
        "# Weights & Biases(wandb)  (optional)\n",
        "%pip install -q wandb\n",
        "import wandb\n",
        "wandb.login()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.7 MB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 133 kB 44.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 180 kB 46.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter: ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msJmk4ZQrQsT"
      },
      "source": [
        "#구글 드라이브에서 yolo모델 데이터셋 폴더로 커스텀 데이터 이동.\n",
        "%cp /content/drive/MyDrive/졸업논문/train /content/yolov5/data/images\n",
        "%cp /content/drive/MyDrive/졸업논문/train_labels /content/yolov5/data/labels\n",
        "%cp /content/drive/MyDrive/졸업논문/dataset.yaml /content/yolov5/data"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "vzw30ZRYYiIL",
        "outputId": "1293e2cc-fff8-4708-9278-e8cf4aed8664"
      },
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "torch.cuda.memory_summary(device=None, abbreviated=False)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Allocations           |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsPscZU--40o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ca4d246-6b88-416e-de64-5ecdb2426274"
      },
      "source": [
        "# dataset.yaml에 훈련, valid를 위한 데이터셋 저장 경로 정보 있음.\n",
        "# dataset.yaml에 경로는 절대경로로\n",
        "\n",
        "!python train.py --img 640 --batch 10 --epochs 5 --data dataset.yaml --weights yolov5x.pt --cache"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mseongkyu\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5x.pt, cfg=, data=dataset.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=5, batch_size=10, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=runs/train, entity=None, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias=latest, local_rank=-1, freeze=0, patience=100\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 🚀 v5.0-454-g9febea7 torch 1.9.0+cu102 CUDA:0 (Tesla K80, 11441.1875MB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.2, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mexalted-dew-21\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/seongkyu/YOLOv5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/seongkyu/YOLOv5/runs/3e15k59i\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/yolov5/wandb/run-20210921_080700-3e15k59i\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=22\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      8800  models.common.Focus                     [3, 80, 3]                    \n",
            "  1                -1  1    115520  models.common.Conv                      [80, 160, 3, 2]               \n",
            "  2                -1  4    309120  models.common.C3                        [160, 160, 4]                 \n",
            "  3                -1  1    461440  models.common.Conv                      [160, 320, 3, 2]              \n",
            "  4                -1 12   3285760  models.common.C3                        [320, 320, 12]                \n",
            "  5                -1  1   1844480  models.common.Conv                      [320, 640, 3, 2]              \n",
            "  6                -1 12  13125120  models.common.C3                        [640, 640, 12]                \n",
            "  7                -1  1   7375360  models.common.Conv                      [640, 1280, 3, 2]             \n",
            "  8                -1  1   4099840  models.common.SPP                       [1280, 1280, [5, 9, 13]]      \n",
            "  9                -1  4  19676160  models.common.C3                        [1280, 1280, 4, False]        \n",
            " 10                -1  1    820480  models.common.Conv                      [1280, 640, 1, 1]             \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  4   5332480  models.common.C3                        [1280, 640, 4, False]         \n",
            " 14                -1  1    205440  models.common.Conv                      [640, 320, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  4   1335040  models.common.C3                        [640, 320, 4, False]          \n",
            " 18                -1  1    922240  models.common.Conv                      [320, 320, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  4   4922880  models.common.C3                        [640, 640, 4, False]          \n",
            " 21                -1  1   3687680  models.common.Conv                      [640, 640, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  4  19676160  models.common.C3                        [1280, 1280, 4, False]        \n",
            " 24      [17, 20, 23]  1    181683  models.yolo.Detect                      [22, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [320, 640, 1280]]\n",
            "Model Summary: 607 layers, 87385683 parameters, 87385683 gradients, 217.8 GFLOPs\n",
            "\n",
            "Transferred 788/794 items from yolov5x.pt\n",
            "Scaled weight_decay = 0.00046875\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 131 weight, 134 weight (no decay), 134 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/yolov5/data/labels/train.cache' images and labels... 7 found, 26 missing, 0 empty, 0 corrupted: 100% 33/33 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.0GB ram): 100% 33/33 [00:00<00:00, 35.75it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/yolov5/data/labels/valid.cache' images and labels... 5 found, 0 missing, 0 empty, 0 corrupted: 100% 5/5 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram): 100% 5/5 [00:00<00:00, 11.73it/s]\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "Plotting labels... \n",
            "\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 4.81, Best Possible Recall (BPR) = 1.0000\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp10\u001b[0m\n",
            "Starting training for 5 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       0/4      9.6G    0.1076   0.03917   0.07499        39       640: 100% 4/4 [00:27<00:00,  6.83s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:01<00:00,  1.26s/it]\n",
            "                 all          5          0          0          0          0          0\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       1/4      9.6G    0.1063   0.03432   0.07461        13       640: 100% 4/4 [00:21<00:00,  5.42s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:01<00:00,  1.29s/it]\n",
            "                 all          5          0          0          0          0          0\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       2/4      9.6G   0.08924   0.03259   0.06124        18       640: 100% 4/4 [00:21<00:00,  5.43s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:01<00:00,  1.28s/it]\n",
            "                 all          5          0          0          0          0          0\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       3/4      9.6G    0.1168   0.02971   0.08007         9       640: 100% 4/4 [00:21<00:00,  5.38s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:01<00:00,  1.28s/it]\n",
            "                 all          5          0          0          0          0          0\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       4/4      9.6G    0.1049    0.0404   0.07374        30       640: 100% 4/4 [00:21<00:00,  5.36s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:01<00:00,  1.49s/it]\n",
            "                 all          5          0          0          0          0          0\n",
            "\n",
            "5 epochs completed in 0.042 hours.\n",
            "Optimizer stripped from runs/train/exp10/weights/last.pt, 175.3MB\n",
            "Optimizer stripped from runs/train/exp10/weights/best.pt, 175.3MB\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 2846\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/yolov5/wandb/run-20210921_080700-3e15k59i/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/yolov5/wandb/run-20210921_080700-3e15k59i/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        metrics/mAP_0.5 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   metrics/mAP_0.5:0.95 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/precision 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         metrics/recall 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/box_loss 0.10487\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/cls_loss 0.07374\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/obj_loss 0.0404\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/box_loss 0.12024\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/cls_loss 0.0811\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/obj_loss 0.12423\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr0 5e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr1 5e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr2 0.09815\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        metrics/mAP_0.5 ▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   metrics/mAP_0.5:0.95 ▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/precision ▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         metrics/recall ▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/box_loss ▆▅▁█▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/cls_loss ▆▆▁█▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/obj_loss ▇▄▃▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/box_loss █▆▄▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/cls_loss █▆▅▃▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/obj_loss ▁▂▄▆█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr0 ▁▆█▇▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr1 ▁▆█▇▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr2 █▆▅▃▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 59 media file(s), 1 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mexalted-dew-21\u001b[0m: \u001b[34mhttps://wandb.ai/seongkyu/YOLOv5/runs/3e15k59i\u001b[0m\n",
            "Results saved to \u001b[1mruns/train/exp10\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnDwKkSN-47A"
      },
      "source": [
        "%rm -rf /content/yolov5/data/labels/train \n",
        "%rm -rf /content/yiolov5/data/labels/valid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNpXZ0n9-lEi"
      },
      "source": [
        "## 3.train을 통해 얻은 best.pt을 이용하여 다시 inference 해보기 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIkF0qv0-7RF"
      },
      "source": [
        "%cp /content/drive/MyDrive/졸업논문/best.pt /content/yolov5"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBdLeXn_AYo5"
      },
      "source": [
        "%cp /content/drive/MyDrive/졸업논문/ARS-CHEL-0.2.mp4 /content/yolov5/data/images/train2"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG2DfaCLCwaO"
      },
      "source": [
        "%cp /content/yolov5/data/labels/train/classes.txt /content/yolov5/data/images/train2"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAoPKEvr-498",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10211b0b-59ec-462e-8752-727d1242588e"
      },
      "source": [
        "!python /content/yolov5/detect.py --weight best.pt --img 640 --conf 0.4  --save-txt --save-crop --source /content/yolov5/data/images/train"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['best.pt'], source=/content/yolov5/data/images/train, imgsz=[640, 640], conf_thres=0.4, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=True, save_conf=False, save_crop=True, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False\n",
            "YOLOv5 🚀 v5.0-454-g9febea7 torch 1.9.0+cu102 CUDA:0 (Tesla K80, 11441.1875MB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 476 layers, 87340003 parameters, 0 gradients, 217.5 GFLOPs\n",
            "image 1/33 /content/yolov5/data/images/train/frame102.jpg: 384x640 Done. (0.158s)\n",
            "image 2/33 /content/yolov5/data/images/train/frame39.jpg: 384x640 Done. (0.151s)\n",
            "image 3/33 /content/yolov5/data/images/train/frame41.jpg: 384x640 Done. (0.138s)\n",
            "image 4/33 /content/yolov5/data/images/train/frame44.jpg: 384x640 Done. (0.138s)\n",
            "image 5/33 /content/yolov5/data/images/train/frame45.jpg: 384x640 Done. (0.138s)\n",
            "image 6/33 /content/yolov5/data/images/train/frame46.jpg: 384x640 Done. (0.130s)\n",
            "image 7/33 /content/yolov5/data/images/train/frame47.jpg: 384x640 Done. (0.128s)\n",
            "image 8/33 /content/yolov5/data/images/train/frame49.jpg: 384x640 Done. (0.128s)\n",
            "image 9/33 /content/yolov5/data/images/train/frame50.jpg: 384x640 Done. (0.127s)\n",
            "image 10/33 /content/yolov5/data/images/train/frame52.jpg: 384x640 Done. (0.125s)\n",
            "image 11/33 /content/yolov5/data/images/train/frame54.jpg: 384x640 Done. (0.127s)\n",
            "image 12/33 /content/yolov5/data/images/train/frame56.jpg: 384x640 Done. (0.128s)\n",
            "image 13/33 /content/yolov5/data/images/train/frame57.jpg: 384x640 Done. (0.128s)\n",
            "image 14/33 /content/yolov5/data/images/train/frame58.jpg: 384x640 Done. (0.128s)\n",
            "image 15/33 /content/yolov5/data/images/train/frame60.jpg: 384x640 Done. (0.128s)\n",
            "image 16/33 /content/yolov5/data/images/train/frame61.jpg: 384x640 Done. (0.128s)\n",
            "image 17/33 /content/yolov5/data/images/train/frame63.jpg: 384x640 Done. (0.130s)\n",
            "image 18/33 /content/yolov5/data/images/train/frame64.jpg: 384x640 Done. (0.133s)\n",
            "image 19/33 /content/yolov5/data/images/train/frame65.jpg: 384x640 Done. (0.133s)\n",
            "image 20/33 /content/yolov5/data/images/train/frame67.jpg: 384x640 Done. (0.133s)\n",
            "image 21/33 /content/yolov5/data/images/train/frame68.jpg: 384x640 Done. (0.133s)\n",
            "image 22/33 /content/yolov5/data/images/train/frame69.jpg: 384x640 Done. (0.133s)\n",
            "image 23/33 /content/yolov5/data/images/train/frame71.jpg: 384x640 Done. (0.130s)\n",
            "image 24/33 /content/yolov5/data/images/train/frame74.jpg: 384x640 Done. (0.128s)\n",
            "image 25/33 /content/yolov5/data/images/train/frame75.jpg: 384x640 Done. (0.128s)\n",
            "image 26/33 /content/yolov5/data/images/train/frame77.jpg: 384x640 Done. (0.127s)\n",
            "image 27/33 /content/yolov5/data/images/train/frame78.jpg: 384x640 Done. (0.125s)\n",
            "image 28/33 /content/yolov5/data/images/train/frame80.jpg: 384x640 Done. (0.127s)\n",
            "image 29/33 /content/yolov5/data/images/train/frame82.jpg: 384x640 Done. (0.127s)\n",
            "image 30/33 /content/yolov5/data/images/train/frame84.jpg: 384x640 Done. (0.128s)\n",
            "image 31/33 /content/yolov5/data/images/train/frame85.jpg: 384x640 Done. (0.128s)\n",
            "image 32/33 /content/yolov5/data/images/train/frame89.jpg: 384x640 Done. (0.126s)\n",
            "image 33/33 /content/yolov5/data/images/train/frame90.jpg: 384x640 Done. (0.127s)\n",
            "Speed: 0.5ms pre-process, 131.0ms inference, 0.5ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp10\u001b[0m\n",
            "0 labels saved to runs/detect/exp10/labels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "873I5mEqp_UG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}